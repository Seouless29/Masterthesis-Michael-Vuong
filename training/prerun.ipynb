{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2216082a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b74eff82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.path.abspath(os.path.join(\"..\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "38579aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.model_uniform import UniformGaborCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dcb8e46e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Post-Gabor shape: torch.Size([2, 40, 96, 96])  expected channels: 40\n"
     ]
    }
   ],
   "source": [
    "# check if the model actually has expected number of wavelets after initialization\n",
    "net = UniformGaborCNN(use_dilation=False)\n",
    "x = torch.randn(2,1,96,96)\n",
    "\n",
    "with torch.no_grad():\n",
    "    z = net.act(net.norm(net.gabor(x)))\n",
    "print(\"Post-Gabor shape:\", z.shape, \" expected channels:\", net.gabor.number_of_wavelets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bdd6a5d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "buffer requires_grad: False\n"
     ]
    }
   ],
   "source": [
    "# check if the weight buffer requires grad. it should not, because of the implementation in gaborfilterfrequency.py\n",
    "def reset_weight_buffer(g):\n",
    "    with torch.no_grad():\n",
    "        if 'weight' in g._parameters:\n",
    "            del g._parameters['weight']\n",
    "        if 'weight' in g._buffers:\n",
    "            del g._buffers['weight']\n",
    "        H, W = g.kernel_size\n",
    "        buf = torch.empty(g.out_channels, H, W, device=g.sigma.device, dtype=g.sigma.dtype)\n",
    "        g.register_buffer('weight', buf)  \n",
    "\n",
    "reset_weight_buffer(net.gabor)\n",
    "\n",
    "print('buffer requires_grad:', getattr(net.gabor.weight, 'requires_grad', None)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3055f489",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‖grad sigma‖: 0.01282531674951315\n",
      "‖grad k‖    : 1.101483702659607\n"
     ]
    }
   ],
   "source": [
    "# check if the sigma and k's change with one simple forward\n",
    "\n",
    "test_model = UniformGaborCNN(image_size=(96,96), num_classes=14).cuda() \n",
    "test_model.train()\n",
    "\n",
    "x = torch.randn(8, 1, 96, 96, device='cuda')\n",
    "y = torch.randint(0, 14, (8,), device='cuda')\n",
    "\n",
    "opt = torch.optim.Adam(test_model.parameters(), lr=1e-3)\n",
    "opt.zero_grad()\n",
    "\n",
    "logits = test_model(x)\n",
    "loss = torch.nn.functional.cross_entropy(logits, y)\n",
    "loss.backward()\n",
    "\n",
    "sig_grad = getattr(test_model.gabor.sigma, 'grad', None)\n",
    "freq_grad = getattr(test_model.gabor.frequency, 'grad', None)\n",
    "print('‖grad sigma‖:', float(sig_grad.norm()) if sig_grad is not None else None)\n",
    "print('‖grad k‖    :', float(freq_grad.norm()) if freq_grad is not None else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "12deaf91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is param: False\n",
      "is buffer: True\n",
      "requires_grad: False\n"
     ]
    }
   ],
   "source": [
    "print(\"is param:\", \"weight\" in net.gabor._parameters) # this should not be a parameter for the optimizer \n",
    "print(\"is buffer:\", \"weight\" in net.gabor._buffers) # instead it is registered as a buffer\n",
    "print(\"requires_grad:\", getattr(net.gabor.weight, \"requires_grad\", None)) # therefore it should not require grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c29758",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
